{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --user ax-platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import scipy\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import gc\n",
    "import sys\n",
    "import uuid\n",
    "import functools\n",
    "import gc\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ax import *\n",
    "from ax.plot.scatter import plot_fitted\n",
    "from ax.utils.notebook.plotting import render, init_notebook_plotting\n",
    "from ax.utils.stats.statstools import agresti_coull_sem\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "feature_name = [\"isEnr\", \"channel\", \"tDrift\", \"avse\", \"dcr\",\"noise\",\"active mass\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting reproducability\n",
    "manualSeed = 158138\n",
    "\n",
    "np.random.seed(manualSeed)\n",
    "random.seed(manualSeed)\n",
    "\n",
    "FIRST_ARM=1\n",
    "IT_ARM=1\n",
    "ITRATION=100\n",
    "BKG_FACTOR = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial(pseed, dataset_tuple):\n",
    "    with open('data.json') as f:\n",
    "        params = json.load(f)\n",
    "    params[\"seed\"] = pseed[\"seed\"]\n",
    "    params[\"bagging_seed\"] = pseed[\"bagging_seed\"]\n",
    "    params[\"neg_bagging_fraction\"] *= params[\"pos_bagging_fraction\"]\n",
    "\n",
    "    lgb_train, lgb_eval, X_test, Y_test = dataset_tuple\n",
    "\n",
    "\n",
    "    # In[137]:\n",
    "\n",
    "\n",
    "    gbm = lgb.train(params,lgb_train,valid_sets=lgb_eval,early_stopping_rounds=3, categorical_feature=[\"isEnr\",\"channel\"])\n",
    "\n",
    "\n",
    "    # In[138]:\n",
    "\n",
    "\n",
    "    y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "\n",
    "    rg=np.arange(0.0,1.0,0.01)\n",
    "    plt.hist(y_pred[Y_test==1], label=\"Signal\", bins=rg, histtype=\"step\", density=True)\n",
    "    plt.hist(y_pred[Y_test==0], label=\"Background\",bins=rg, histtype=\"step\", density=True)\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"BDT output\")\n",
    "    plt.ylabel(\"% per 0.01 bin(a.u.)\")\n",
    "    plt.savefig(\"BDT_output.png\")\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "    fpr, tpr, thr = roc_curve(Y_test, y_pred)\n",
    "    fpra, tpra, thra= roc_curve(Y_test,  X_test[:,3])\n",
    "    \n",
    "    avsecut = np.argmin(np.abs(thra+1.0))\n",
    "    bdtcut = np.argmin(np.abs(tpr-tpra[avsecut]))\n",
    "    performance_improvement = (1-fpr[bdtcut]) - (1-fpra[avsecut])\n",
    "    print(1-fpr[bdtcut],1-fpra[avsecut], performance_improvement)\n",
    "    return performance_improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signaldata = np.load(\"sig.npy\")\n",
    "bkgdata = np.load(\"bkg.npy\")\n",
    "\n",
    "#split signal dataset\n",
    "test_split = 0.3\n",
    "indices = np.arange(signaldata.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "train_index = indices[int(len(indices)*test_split):]\n",
    "test_index = indices[:int(len(indices)*test_split)]\n",
    "signal_train = signaldata[train_index]\n",
    "signal_test = signaldata[test_index]\n",
    "siglabel_train = np.ones(signal_train.shape[0])\n",
    "siglabel_test = np.ones(signal_test.shape[0])\n",
    "\n",
    "#split bkg dataset\n",
    "indices = np.arange(bkgdata.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "train_index = indices[int(len(indices)*test_split):]\n",
    "test_index = indices[:int(len(indices)*test_split)]\n",
    "bkg_train = bkgdata[train_index]\n",
    "bkg_test = bkgdata[test_index]\n",
    "bkglabel_train = np.zeros(bkg_train.shape[0])\n",
    "bkglabel_test = np.zeros(bkg_test.shape[0])\n",
    "\n",
    "#shuffle train dataset\n",
    "X_train = np.concatenate([signal_train, bkg_train],axis = 0)\n",
    "Y_train = np.concatenate([siglabel_train, bkglabel_train],axis = 0)\n",
    "train_index = np.arange(len(X_train))\n",
    "np.random.shuffle(train_index)\n",
    "X_train = X_train[train_index]\n",
    "Y_train = Y_train[train_index]\n",
    "X_test = np.concatenate([signal_test, bkg_test],axis = 0)\n",
    "Y_test = np.concatenate([siglabel_test, bkglabel_test],axis = 0)\n",
    "\n",
    "#split test into valid and test data then shuffle\n",
    "test_index = np.arange(len(X_test))\n",
    "np.random.shuffle(test_index)\n",
    "X_test = X_test[test_index]\n",
    "Y_test = Y_test[test_index]\n",
    "if len(X_test)%2 == 1:\n",
    "    X_test = X_test[:-1]\n",
    "    Y_test = Y_test[:-1]\n",
    "X_val, X_test = np.split(X_test,2)\n",
    "Y_val, Y_test = np.split(Y_test,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(X_train, Y_train,free_raw_data=False, feature_name = fname\n",
    ")\n",
    "lgb_eval = lgb.Dataset(X_val, Y_val, reference=lgb_train,free_raw_data=False, feature_name = fname\n",
    ")\n",
    "dataset_tuple = (lgb_train, lgb_eval, X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of Parameters\n",
    "p1 = RangeParameter(name=\"seed\", lower=0, upper=100000000, parameter_type=ParameterType.INT)\n",
    "p2 = RangeParameter(name=\"bagging_seed\", lower=0, upper=100000000, parameter_type=ParameterType.INT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = SearchSpace(\n",
    "    parameters=[p1,p2],\n",
    ")\n",
    "\n",
    "experiment = Experiment(\n",
    "    name=\"hyper_parameter_optimization\",\n",
    "    search_space=search_space,\n",
    ")\n",
    "\n",
    "sobol = Models.SOBOL(search_space=experiment.search_space)\n",
    "generator_run = sobol.gen(FIRST_ARM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class cd:\n",
    "    '''\n",
    "    Context manager for changing the current working directory\n",
    "    '''\n",
    "    def __init__(self, newPath):\n",
    "        self.newPath = newPath\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.savedPath = os.getcwd()\n",
    "        os.chdir(self.newPath)\n",
    "\n",
    "    def __exit__(self, etype, value, traceback):\n",
    "        os.chdir(self.savedPath)\n",
    "\n",
    "class MyRunner(Runner):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        nothing\n",
    "        '''\n",
    "\n",
    "    def run(self, trial):\n",
    "        arm_result = []\n",
    "        for arm_name, arm in trial.arms_by_name.items():\n",
    "            params = arm.parameters\n",
    "            print(arm.parameters)\n",
    "            # train_loader = data_utils.DataLoader(self.dataset, batch_size=params[\"BATCH_SIZE\"], sampler=self.train_sampler, drop_last=True, num_workers = 0)\n",
    "            # test_loader = data_utils.DataLoader(self.dataset, batch_size=params[\"BATCH_SIZE\"], sampler=self.test_sampler, drop_last=True, num_workers = 0)\n",
    "            auc = run_trial(params, dataset_tuple)\n",
    "            arm_result.append(float(auc))\n",
    "        return {\"name\": str(trial.index), \"auc\": arm_result}\n",
    "\n",
    "class BoothMetric(Metric):\n",
    "    def fetch_trial_data(self, trial):  \n",
    "        records = []\n",
    "        auc_result = trial.run_metadata[\"auc\"]\n",
    "        index = 0\n",
    "        for arm_name, arm in trial.arms_by_name.items():\n",
    "            params = arm.parameters\n",
    "            records.append({\n",
    "                \"arm_name\": arm_name,\n",
    "                \"metric_name\": self.name,\n",
    "                \"mean\": auc_result[index],\n",
    "                \"sem\": 0.0,\n",
    "                \"trial_index\": trial.index\n",
    "            })\n",
    "            index += 1\n",
    "        return Data(df=pd.DataFrame.from_records(records))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#run search\n",
    "# VERSION=\"bdt_sepdep\"\n",
    "# hpsearch_dir = 'hpsearch_' + str(VERSION)\n",
    "# if os.path.exists(hpsearch_dir):\n",
    "#     shutil.rmtree(hpsearch_dir)\n",
    "# os.mkdir(hpsearch_dir)\n",
    "# hpsearch_dir = os.getcwd() + '/' + hpsearch_dir\n",
    "\n",
    "# with cd(hpsearch)\n",
    "experiment.runner = MyRunner()\n",
    "experiment.new_batch_trial(generator_run=generator_run)\n",
    "\n",
    "experiment.trials[0].run()\n",
    "\n",
    "optimization_config = OptimizationConfig(\n",
    "    objective = Objective(\n",
    "        metric=BoothMetric(name=\"booth\"), \n",
    "        minimize=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "experiment.optimization_config = optimization_config\n",
    "metric_eval = []\n",
    "for i in range(1, ITRATION):\n",
    "\n",
    "    data = experiment.fetch_data()\n",
    "    gpei = Models.GPEI(experiment=experiment, data=data)\n",
    "    generator_run = gpei.gen(IT_ARM)\n",
    "    experiment.new_batch_trial(generator_run=generator_run)\n",
    "    experiment.trials[i].run()\n",
    "    data = experiment.fetch_data()\n",
    "    df = data.df\n",
    "    print(df)\n",
    "    best_arm_name = df.arm_name[df['mean'] == df['mean'].max()].values[0]\n",
    "    best_arm = experiment.arms_by_name[best_arm_name]\n",
    "    print(best_arm)\n",
    "    json_field = best_arm.parameters\n",
    "    json_field[\"improvement\"] = df['mean'].max() * 100\n",
    "    metric_eval.append(df['mean'].max() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.array(df[\"mean\"]).shape)\n",
    "plt.hist(np.array(df[\"mean\"])*100, bins=np.linspace(2.4,2.7,30))\n",
    "plt.xlabel(\"BDT Eff. - A vs. E Eff.(%)\")\n",
    "plt.savefig(\"ucert.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-v1.4.0",
   "language": "python",
   "name": "pytorch-v1.4.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
